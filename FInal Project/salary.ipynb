{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwyzaREx-HZC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC4vUs4X-HZD"
      },
      "source": [
        "# LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsPMT2Wq-HZE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "354f39d3-90bc-4b9a-94d9-143f7dfa6d07"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'salary_data_cleaned.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-222c0a655663>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msalaries_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'salary_data_cleaned.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'salary_data_cleaned.csv'"
          ]
        }
      ],
      "source": [
        "salaries_df = pd.read_csv('salary_data_cleaned.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYhmQfuo-HZE"
      },
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch_FIfuv-HZE"
      },
      "outputs": [],
      "source": [
        "salaries_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhQVM1X0-HZF"
      },
      "outputs": [],
      "source": [
        "shape = salaries_df.shape\n",
        "print(f\"Records: {shape[0]}, Features: {shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyNzpSDR-HZF"
      },
      "outputs": [],
      "source": [
        "salaries_df.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WxDsO9i-HZF"
      },
      "outputs": [],
      "source": [
        "categorical_columns = salaries_df.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Print the number of unique values for each categorical column\n",
        "for column in categorical_columns:\n",
        "    print(f'{column} : {len(set(salaries_df[column].unique()))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP-6k7xL-HZF"
      },
      "source": [
        "## Feature Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sETqnKS-HZF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "41e5e82b-5be0-483f-cce3-61aae0058c3c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'salaries_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-12c4380dc5c3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate the correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorrelation_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msalaries_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int64'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'salaries_df' is not defined"
          ]
        }
      ],
      "source": [
        "correlation_matrix = salaries_df.select_dtypes(include=['float64', 'int64']).corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical"
      ],
      "metadata": {
        "id": "QVGYwl4NAW7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These categorical features have over 100 unqiue values so only the first 20 values are plotted.**"
      ],
      "metadata": {
        "id": "JzotIqM_AiGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features_top_20 = ['Job Title', 'Salary Estimate', 'Company Name', 'Location', 'Headquarters', 'Competitors']\n",
        "\n",
        "plt.figure(figsize=(20, 30))\n",
        "\n",
        "# Loop through the categorical features and create a subplot for each\n",
        "for i, feature in enumerate(categorical_features_top_20, start=1):\n",
        "    top_20 = salaries_df[feature].value_counts().head(20) # Get top 20 values for each feature\n",
        "    plt.subplot(3, 2, i)\n",
        "    sns.barplot(x=top_20.values, y=top_20.index, palette='Set3')\n",
        "    plt.title(f'Distribution of Top 20 {feature}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9ywbCMa2AZPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These categorical features have less than 100 unqiue values so all their values are plotted.**"
      ],
      "metadata": {
        "id": "IGu4ykn8EkNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['Size', 'Type of ownership', 'Sector', 'Revenue', 'job_state', 'Industry']\n",
        "\n",
        "plt.figure(figsize=(15, 30))\n",
        "\n",
        "# Loop through the categorical features and create a subplot for each\n",
        "for i, feature in enumerate(categorical_features, start=1):\n",
        "    plt.subplot(3, 2, i)  # 3 rows, 2 columns\n",
        "    # Order the categories by count\n",
        "    ordered_data = salaries_df[feature].value_counts().index\n",
        "    sns.countplot(y=feature, data=salaries_df, order=ordered_data, palette='Set3')\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D-CMNBluEg5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Numerical"
      ],
      "metadata": {
        "id": "0dN2vwUSD_zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These numerical features are binary.**"
      ],
      "metadata": {
        "id": "rkD15zgTFJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = ['hourly', 'employer_provided', 'same_state']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, skill in enumerate(numerical_columns, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    salaries_df[skill].value_counts().plot(kind='bar', color=['lightgreen', 'skyblue'], edgecolor='black')\n",
        "    plt.xticks(ticks=[1, 0], labels=['Yes', 'No'], rotation=0)\n",
        "    plt.ylabel('Number of Jobs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4gtacagCFHGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tech_skills = ['python_yn', 'R_yn', 'spark', 'aws', 'excel']\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, skill in enumerate(tech_skills, 1):\n",
        "    plt.subplot(2, 3, i)\n",
        "    salaries_df[skill].value_counts().plot(kind='bar', color=['lightgreen', 'skyblue'], edgecolor='black')\n",
        "    plt.title(f'Jobs Requiring {skill.capitalize()}')\n",
        "    plt.xticks(ticks=[1, 0], labels=['Yes', 'No'], rotation=0)\n",
        "    plt.ylabel('Number of Jobs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1CZUqqgBFrFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**These numerical features were divided into logical bins (i.e rating->1-5 and founded->decades).**"
      ],
      "metadata": {
        "id": "VCh_5ZAmF9CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out companies with 'Founded' year as -1 (since -1 represents missing or irrelevant data)\n",
        "df_filtered = salaries_df[salaries_df['Founded'] > 0]\n",
        "\n",
        "# Create a new column 'Decade' categorizing 'Founded' into decades\n",
        "# The decade is calculated by dividing the year by 10, converting to integer, and then multiplying by 10\n",
        "df_filtered['Decade'] = ((df_filtered['Founded'] // 10) * 10).astype(int)\n",
        "\n",
        "# Count the number of companies founded in each decade\n",
        "decade_counts = df_filtered['Decade'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "decade_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
        "plt.title('Distribution of Companies Founded by Decade')\n",
        "plt.xlabel('Decade')\n",
        "plt.ylabel('Number of Companies')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "msMK6tW8EBwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range of bins from the minimum to maximum rating,\n",
        "# since ratings are between 0 and 5, and include an extra bin for -1 (unrated or missing ratings)\n",
        "bins = [-1.5] + list(range(int(salaries_df['Rating'].min()), int(salaries_df['Rating'].max()) + 2))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(salaries_df['Rating'], bins=bins, color=\"skyblue\", edgecolor='black', alpha=0.7)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Ratings')\n",
        "plt.xticks(range(int(salaries_df['Rating'].min()), int(salaries_df['Rating'].max()) + 1))  # Setting x-ticks to integer ratings\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "x_diOb0uEThV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYmzsRdW-HZG"
      },
      "source": [
        "## Feature Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8pzSnzQ-HZG"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = salaries_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiV_RIrl-HZG"
      },
      "outputs": [],
      "source": [
        "categorical_features_to_remove = [\n",
        "    \"Salary Estimate\",\n",
        "    \"Job Description\",\n",
        "    \"Headquarters\",\n",
        "    \"Size\",\n",
        "    \"Industry\",\n",
        "    \"Competitors\",\n",
        "    \"company_txt\",\n",
        "    \"job_state\"\n",
        "]\n",
        "\n",
        "numerical_features_to_remove = [\n",
        "    \"R_yn\",\n",
        "    \"hourly\",\n",
        "    \"employer_provided\",\n",
        "    \"Founded\",\n",
        "    \"Rating\",\n",
        "    \"age\",\n",
        "    \"same_state\"\n",
        "]\n",
        "\n",
        "features_to_remove = categorical_features_to_remove + numerical_features_to_remove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KUZOXH3-HZG"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = preprocessed_salaries_df.drop(features_to_remove, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHcl_eJB-HZG"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bymxnn5W-HZG"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdyby-tV-HZG"
      },
      "source": [
        "### Job Title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdffUelo-HZG"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEotF9aS-HZG"
      },
      "outputs": [],
      "source": [
        "salaries_df['Job Title'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKLQ0YuN-HZG"
      },
      "outputs": [],
      "source": [
        "def categorize_job_titles(df):\n",
        "    # Define the regex patterns and their corresponding column names\n",
        "    regex_to_job_titles = {\n",
        "        \"Data\": \"data\",\n",
        "        \"(Scientist|Scientists|Researcher|Science|Research|R&D)\": \"scientist\",\n",
        "        \"(Engineer|Developer|R&D|Development)\": \"engineer\",\n",
        "        \"(Analyst|Analysis|Analytics)\": \"analyst\",\n",
        "        \"(Senior|Director|VP|Manager|Principal|Sr\\.|Sr|Staff|Associate|Jr\\.|Jr)\": \"manager\"\n",
        "    }\n",
        "\n",
        "    # Initialize a DataFrame to hold the counts\n",
        "    counts_df = pd.DataFrame(index=salaries_df.index)\n",
        "\n",
        "    # Initialize columns in counts_df with zeros\n",
        "    for keyword in regex_to_job_titles.values():\n",
        "        counts_df[keyword] = 0\n",
        "\n",
        "    # Loop through each job title and update counts\n",
        "    for index, row in df.iterrows():\n",
        "        job_title = row['Job Title']\n",
        "        for pattern, keyword in regex_to_job_titles.items():\n",
        "            if re.search(pattern, job_title, re.IGNORECASE):\n",
        "                counts_df.at[index, keyword] = 1\n",
        "\n",
        "    return counts_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a__ggq8-HZG"
      },
      "outputs": [],
      "source": [
        "# Test the categorize_job_titles() with a job title from the dataset\n",
        "job_titles_count_df = categorize_job_titles(preprocessed_salaries_df)\n",
        "\n",
        "print(f'Job Title: {preprocessed_salaries_df.loc[73, \"Job Title\"]}\\n')\n",
        "print(f'Word Count: \\n{job_titles_count_df.loc[73]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "py6TfjJH-HZG"
      },
      "outputs": [],
      "source": [
        " preprocessed_salaries_df = pd.concat([preprocessed_salaries_df.drop('Job Title', axis=1), job_titles_count_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_XuyvI--HZG"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8yytu41-HZG"
      },
      "source": [
        "### Location"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The commented out code was run locally to obtain the longitude and latitude coordinates of each location. Since it takes a while to get all the locations from the GoogleMap API, the locations were saved in a seperate csv file location_coordinates.csv"
      ],
      "metadata": {
        "id": "T3A6vfPe5GQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgJXP994-HZH"
      },
      "outputs": [],
      "source": [
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "# from googlemaps import Client as GoogleMaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WonJQg-B-HZH"
      },
      "outputs": [],
      "source": [
        "# load_dotenv()\n",
        "\n",
        "# GOOGLE_MAPS_API_KEY = os.getenv('GOOGLE_MAPS_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKfFsVO8-HZH"
      },
      "outputs": [],
      "source": [
        "# gmaps = GoogleMaps(GOOGLE_MAPS_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9KnmwM5-HZH"
      },
      "outputs": [],
      "source": [
        "# def geocode_location(location):\n",
        "#     geocode_result = gmaps.geocode(location)\n",
        "#     if geocode_result:\n",
        "#         latitude = geocode_result[0]['geometry']['location']['lat']\n",
        "#         longitude = geocode_result[0]['geometry']['location']['lng']\n",
        "#         return latitude, longitude\n",
        "#     else:\n",
        "#         return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Gj6Li6-HZH"
      },
      "outputs": [],
      "source": [
        "# # Test the geocode_location() with a location from the dataset\n",
        "# location_example = salaries_df['Location'][0]\n",
        "# print(f\"Location: {location_example}\")\n",
        "\n",
        "# print(f\"Latitude, Longitude: {geocode_location(location_example)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMmUm3BZ-HZH"
      },
      "outputs": [],
      "source": [
        "# location_df = salaries_df['Location'].apply(geocode_location).apply(pd.Series)\n",
        "\n",
        "# location_df.columns = ['latitude', 'longitude']\n",
        "# print(location_df)\n",
        "\n",
        "# location_df.to_csv('location_coordinates.csv', index=False) # Save the location coordinates to a CSV file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNGS481t-HZH"
      },
      "outputs": [],
      "source": [
        "location_coordinates_df = pd.read_csv('location_coordinates.csv')\n",
        "location_coordinates_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3HU1My0-HZH"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = pd.concat([preprocessed_salaries_df.drop('Location', axis=1), location_coordinates_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS6nfMsJ-HZH"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW_ilP8W-HZH"
      },
      "source": [
        "### Sector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDPIJVGh-HZH"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df['Sector'] = preprocessed_salaries_df['Sector'].replace('-1', 'Other')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlqsPKw0-HZH"
      },
      "outputs": [],
      "source": [
        "salaries_df['Sector'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRKdXEEk-HZH"
      },
      "outputs": [],
      "source": [
        "sector_df = pd.get_dummies(preprocessed_salaries_df['Sector'], prefix='sector')\n",
        "sector_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "730sKLPy-HZP"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = pd.concat([preprocessed_salaries_df.drop('Sector', axis=1), sector_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_X20eDN-HZP"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgml5J5C-HZP"
      },
      "source": [
        "### Ownership"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qKCkncW-HZP"
      },
      "outputs": [],
      "source": [
        "salaries_df['Type of ownership'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_0QARxu-HZP"
      },
      "outputs": [],
      "source": [
        "def encode_ownership(row):\n",
        "    ownership_types = {\n",
        "        \"Company - Private\": 0,\n",
        "        \"Company - Public\": 0,\n",
        "        \"Nonprofit Organization\": 0,\n",
        "        \"Subsidiary or Business Segment\": 0,\n",
        "        \"Government\": 0,\n",
        "        \"Other\": 0\n",
        "    }\n",
        "\n",
        "    # Check if the row's ownership type matches one of the predefined types\n",
        "    if row['Type of ownership'] in ownership_types:\n",
        "        ownership_types[row['Type of ownership']] = 1\n",
        "    else:\n",
        "        # If it does not match, categorize as \"Other\"\n",
        "        ownership_types[\"Other\"] = 1\n",
        "    return pd.Series(ownership_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcZmqqQ--HZP"
      },
      "outputs": [],
      "source": [
        "ownership_df = preprocessed_salaries_df.apply(encode_ownership, axis=1)\n",
        "ownership_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4IsNAVu-HZP"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = pd.concat([preprocessed_salaries_df.drop('Type of ownership', axis=1), ownership_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJwUk9xx-HZP"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLDsQqz--HZP"
      },
      "source": [
        "### Revenue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXimRP5d-HZP"
      },
      "outputs": [],
      "source": [
        "#Revenue\n",
        "def encode_revenue(revenue):\n",
        "    if revenue == \"$10+billion (USD)\":\n",
        "        return 10000000000, 10000000000  # Min and Max are both 10 billion\n",
        "\n",
        "    revenue = revenue.replace('(USD)', '').replace('$', '').strip()\n",
        "    if 'to' in revenue:\n",
        "        min_rev, max_rev = revenue.split(' to ')\n",
        "        min_rev = float(min_rev.replace(' million', '').replace(' billion', '').replace('+', '')) * (1000000 if 'million' in min_rev else 1000000000)\n",
        "        max_rev = float(max_rev.replace(' million', '').replace(' billion', '').strip()) * (1000000 if 'million' in max_rev else 1000000000)\n",
        "        return min_rev, max_rev\n",
        "    elif 'Less than' in revenue:\n",
        "        return 0, 1000000\n",
        "    elif 'Unknown' in revenue or '-1' in revenue:\n",
        "        return None, None\n",
        "    else:\n",
        "        # Handles cases with \"+\", assuming it refers to values at or above the mentioned figure\n",
        "        if '+' in revenue:\n",
        "            value = float(revenue.replace(' million', '').replace(' billion', '').replace('+', ''))\n",
        "            value = value * (1000000 if 'million' in revenue else 1000000000)\n",
        "            return value, value\n",
        "        else:\n",
        "            return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LVnI2dM-HZP"
      },
      "outputs": [],
      "source": [
        "revenue_df = preprocessed_salaries_df['Revenue'].apply(lambda x: pd.Series(encode_revenue(x)))\n",
        "revenue_df.columns = ['min_revenue', 'max_revenue']\n",
        "revenue_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05r5wPGA-HZP"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = pd.concat([preprocessed_salaries_df.drop('Revenue', axis=1), revenue_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filling Na for Unknown instances for 'min_revenue', 'max_revenue'\n",
        "# Assuming 'df' is your DataFrame\n",
        "# Calculate the median of the 'min_revenue' column, excluding NaN values\n",
        "min_revenue_median = preprocessed_salaries_df['min_revenue'].median()\n",
        "\n",
        "# Fill NaN values in the 'min_revenue' column with the calculated median\n",
        "preprocessed_salaries_df['min_revenue'] = preprocessed_salaries_df['min_revenue'].fillna(min_revenue_median)\n",
        "\n",
        "# Calculate the median of the 'max_revenue' column, excluding NaN values\n",
        "max_revenue_median = preprocessed_salaries_df['max_revenue'].median()\n",
        "\n",
        "# Fill NaN values in the 'max_revenue' column with the calculated median\n",
        "preprocessed_salaries_df['max_revenue'] = preprocessed_salaries_df['max_revenue'].fillna(max_revenue_median)\n",
        "\n",
        "# This updates 'min_revenue' and 'max_revenue' columns in df with NaN values replaced by their respective medians\n"
      ],
      "metadata": {
        "id": "U-3vrv2YPkym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhiIg8jV-HZP"
      },
      "outputs": [],
      "source": [
        "print(f\"Records: {preprocessed_salaries_df.shape[0]}, Features: {preprocessed_salaries_df.shape[1]}\")\n",
        "preprocessed_salaries_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqWhXV8J-HZQ"
      },
      "outputs": [],
      "source": [
        "preprocessed_salaries_df = preprocessed_salaries_df.drop(['Company Name'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_salaries_df.dtypes"
      ],
      "metadata": {
        "id": "XEJRKV_zBsEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nqIpJ63MBuuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalizing"
      ],
      "metadata": {
        "id": "23e7-lNtPoPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "1Km5oeVdlOwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = ['latitude', 'longitude', 'min_revenue', 'max_revenue']\n",
        "binary_features = ['python_yn', 'spark', 'aws', 'excel', 'data', 'scientist', 'engineer', 'analyst', 'manager',\n",
        "                   'sector_Accounting & Legal', 'sector_Aerospace & Defense', 'sector_Agriculture & Forestry',\n",
        "                   'sector_Arts, Entertainment & Recreation', 'sector_Biotech & Pharmaceuticals', 'sector_Business Services',\n",
        "                   'sector_Construction, Repair & Maintenance', 'sector_Consumer Services', 'sector_Education',\n",
        "                   'sector_Finance', 'sector_Government', 'sector_Health Care', 'sector_Information Technology',\n",
        "                   'sector_Insurance', 'sector_Manufacturing', 'sector_Media', 'sector_Mining & Metals', 'sector_Non-Profit',\n",
        "                   'sector_Oil, Gas, Energy & Utilities', 'sector_Other', 'sector_Real Estate', 'sector_Retail',\n",
        "                   'sector_Telecommunications', 'sector_Transportation & Logistics', 'sector_Travel & Tourism',\n",
        "                   'Company - Private', 'Company - Public', 'Nonprofit Organization', 'Subsidiary or Business Segment',\n",
        "                   'Government', 'Other']\n"
      ],
      "metadata": {
        "id": "-gUagggQPqs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Scale numerical features\n",
        "preprocessed_salaries_df[numerical_features] = scaler.fit_transform(preprocessed_salaries_df[numerical_features])\n"
      ],
      "metadata": {
        "id": "dZLQJpK-Pt5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_salaries_df"
      ],
      "metadata": {
        "id": "ksBCwAcJPt8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "na_count_column = preprocessed_salaries_df['min_revenue'].isna().sum()\n",
        "\n",
        "print(f\"Number of NaNs in the column: {na_count_column}\")"
      ],
      "metadata": {
        "id": "7OhgADS-Pt-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KNN model"
      ],
      "metadata": {
        "id": "vswuKLhcQkuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Assuming preprocessed_salaries_df is your DataFrame\n",
        "X = preprocessed_salaries_df.drop(['min_salary', 'max_salary', 'avg_salary'], axis=1)\n",
        "y = preprocessed_salaries_df[['min_salary', 'max_salary', 'avg_salary']]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a range of k values to try\n",
        "k_values = range(1, 30)\n",
        "best_rmse = np.inf\n",
        "best_k = 0\n",
        "\n",
        "# Loop over each k value, train the model, and compute the RMSE on the testing set\n",
        "for k in k_values:\n",
        "    knn = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the testing set\n",
        "    test_predictions = knn.predict(X_test)\n",
        "\n",
        "    # Calculate average RMSE for the testing set\n",
        "    avg_rmse_test = np.mean([\n",
        "        mean_squared_error(y_test['min_salary'], test_predictions[:, 0], squared=False),\n",
        "        mean_squared_error(y_test['max_salary'], test_predictions[:, 1], squared=False),\n",
        "        mean_squared_error(y_test['avg_salary'], test_predictions[:, 2], squared=False)\n",
        "    ])\n",
        "\n",
        "    # Update the best_k if the current average RMSE is lower\n",
        "    if avg_rmse_test < best_rmse:\n",
        "        best_rmse = avg_rmse_test\n",
        "        best_k = k\n",
        "\n",
        "# Re-train the model with the best_k found\n",
        "knn_best = KNeighborsRegressor(n_neighbors=best_k)\n",
        "knn_best.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with the best model for both training and testing sets\n",
        "train_predictions_best = knn_best.predict(X_train)\n",
        "test_predictions_best = knn_best.predict(X_test)\n",
        "\n",
        "# Calculate RMSE for the training and testing sets with the best model\n",
        "# Training set\n",
        "rmse_train_min = mean_squared_error(y_train['min_salary'], train_predictions_best[:, 0], squared=False)\n",
        "rmse_train_max = mean_squared_error(y_train['max_salary'], train_predictions_best[:, 1], squared=False)\n",
        "rmse_train_avg = mean_squared_error(y_train['avg_salary'], train_predictions_best[:, 2], squared=False)\n",
        "\n",
        "# Testing set\n",
        "rmse_test_min = mean_squared_error(y_test['min_salary'], test_predictions_best[:, 0], squared=False)\n",
        "rmse_test_max = mean_squared_error(y_test['max_salary'], test_predictions_best[:, 1], squared=False)\n",
        "rmse_test_avg = mean_squared_error(y_test['avg_salary'], test_predictions_best[:, 2], squared=False)\n",
        "\n",
        "# Print out the best K value and RMSE for both sets\n",
        "print(f\"Best K Value: {best_k}\")\n",
        "print(\"\\nTraining Set RMSE with Best K:\")\n",
        "print(f\"Min Salary: {rmse_train_min}\")\n",
        "print(f\"Max Salary: {rmse_train_max}\")\n",
        "print(f\"Avg Salary: {rmse_train_avg}\")\n",
        "\n",
        "print(\"\\nTesting Set RMSE with Best K:\")\n",
        "print(f\"Min Salary: {rmse_test_min}\")\n",
        "print(f\"Max Salary: {rmse_test_max}\")\n",
        "print(f\"Avg Salary: {rmse_test_avg}\")\n"
      ],
      "metadata": {
        "id": "ZHIf0lLTPuBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CP-6k7xL-HZF",
        "LYmzsRdW-HZG",
        "PW_ilP8W-HZH",
        "Vgml5J5C-HZP"
      ]
    },
    "kernelspec": {
      "display_name": "csci183",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}